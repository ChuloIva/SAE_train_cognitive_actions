{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SAE Interpretation with Local LLM\n",
    "This notebook interprets all neurons in a trained SAE using a local Gemma 3 12B model via vLLM."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup: Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clone HypotheSAEs if not already present\n",
    "!git clone https://github.com/rmovva/HypotheSAEs.git 2>/dev/null || echo \"HypotheSAEs already exists\"\n",
    "!pip install -e HypotheSAEs/\n",
    "\n",
    "# Install vLLM and other dependencies\n",
    "!pip install vllm transformers accelerate huggingface_hub"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load SAE from HuggingFace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, \"HypotheSAEs\")\n",
    "\n",
    "from hypothesaes.sae import load_model\n",
    "import torch\n",
    "import numpy as np\n",
    "from huggingface_hub import snapshot_download\n",
    "import os\n",
    "\n",
    "# Configuration\n",
    "SAE_REPO = \"Koalacrown/llama3.1-8b-it-cognitive-actions-sae-l11\"\n",
    "LOCAL_SAE_DIR = \"sae_checkpoint\"\n",
    "\n",
    "print(f\"Downloading SAE from {SAE_REPO}...\")\n",
    "snapshot_download(\n",
    "    repo_id=SAE_REPO,\n",
    "    local_dir=LOCAL_SAE_DIR,\n",
    "    repo_type=\"model\"\n",
    ")\n",
    "\n",
    "# Find the SAE checkpoint file\n",
    "sae_files = [f for f in os.listdir(LOCAL_SAE_DIR) if f.startswith('SAE_') and f.endswith('.pt')]\n",
    "if not sae_files:\n",
    "    raise FileNotFoundError(f\"No SAE checkpoint found in {LOCAL_SAE_DIR}\")\n",
    "\n",
    "sae_path = os.path.join(LOCAL_SAE_DIR, sae_files[0])\n",
    "print(f\"Loading SAE from {sae_path}...\")\n",
    "\n",
    "sae = load_model(sae_path)\n",
    "print(f\"SAE loaded: M={sae.m_total_neurons}, K={sae.k_active_neurons}\")\n",
    "print(f\"Input dimension: {sae.input_dim}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Dataset for Interpretation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Load the cognitive actions dataset\n",
    "DATASET_PATH = \"cognitive_actions_7k_final_1759233061.jsonl\"\n",
    "\n",
    "def load_dataset(dataset_path: str):\n",
    "    \"\"\"Load JSONL dataset.\"\"\"\n",
    "    data = []\n",
    "    with open(dataset_path, 'r') as f:\n",
    "        for line in f:\n",
    "            data.append(json.loads(line))\n",
    "    return data\n",
    "\n",
    "print(f\"Loading dataset from {DATASET_PATH}...\")\n",
    "data = load_dataset(DATASET_PATH)\n",
    "texts = [item['text'] for item in data]\n",
    "print(f\"Loaded {len(texts)} examples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract Activations from Base Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "def extract_activations_sequential(\n",
    "    texts,\n",
    "    model_name=\"meta-llama/Llama-3.1-8B-Instruct\",\n",
    "    layer_idx=11,\n",
    "    max_length=512,\n",
    "    device=\"cuda\" if torch.cuda.is_available() else \"cpu\",\n",
    "):\n",
    "    \"\"\"Extract activations from LLM sequentially.\"\"\"\n",
    "    print(f\"Loading model: {model_name}\")\n",
    "    print(f\"Extracting from layer {layer_idx}\")\n",
    "    \n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "    if tokenizer.pad_token is None:\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "    \n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_name,\n",
    "        torch_dtype=torch.float16,\n",
    "        device_map=\"auto\",\n",
    "        trust_remote_code=True\n",
    "    )\n",
    "    model.eval()\n",
    "    \n",
    "    all_activations = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for text in tqdm(texts, desc=\"Extracting activations\"):\n",
    "            inputs = tokenizer(\n",
    "                text,\n",
    "                return_tensors=\"pt\",\n",
    "                truncation=True,\n",
    "                max_length=max_length,\n",
    "                padding=False,\n",
    "            ).to(device)\n",
    "            \n",
    "            outputs = model(**inputs, output_hidden_states=True, return_dict=True)\n",
    "            layer_activations = outputs.hidden_states[layer_idx]\n",
    "            layer_activations = layer_activations.squeeze(0).cpu().float().numpy()\n",
    "            all_activations.append(layer_activations)\n",
    "            \n",
    "            del inputs, outputs, layer_activations\n",
    "            if torch.cuda.is_available():\n",
    "                torch.cuda.empty_cache()\n",
    "    \n",
    "    return all_activations\n",
    "\n",
    "# Extract activations\n",
    "MODEL_NAME = \"meta-llama/Llama-3.1-8B-Instruct\"\n",
    "LAYER_IDX = 11\n",
    "\n",
    "activations_list = extract_activations_sequential(\n",
    "    texts=texts,\n",
    "    model_name=MODEL_NAME,\n",
    "    layer_idx=LAYER_IDX,\n",
    "    max_length=512,\n",
    ")\n",
    "\n",
    "# Flatten activations (simple concatenation for interpretation)\n",
    "print(\"Flattening activations...\")\n",
    "flattened_activations = np.vstack(activations_list)\n",
    "print(f\"Flattened shape: {flattened_activations.shape}\")\n",
    "\n",
    "# Flatten texts to match activations (repeat each text for its sequence length)\n",
    "flattened_texts = []\n",
    "for i, acts in enumerate(activations_list):\n",
    "    flattened_texts.extend([texts[i]] * acts.shape[0])\n",
    "print(f\"Total text segments: {len(flattened_texts)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-load Local Model with vLLM (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hypothesaes.llm_local import get_vllm_engine\n",
    "\n",
    "# Use Gemma 3 12B for interpretation\n",
    "INTERPRETER_MODEL = \"google/gemma-2-12b-it\"\n",
    "\n",
    "print(f\"Pre-loading vLLM with {INTERPRETER_MODEL}...\")\n",
    "print(\"(This is optional - the model will auto-load on first use if skipped)\")\n",
    "\n",
    "# Pre-load with custom vLLM settings\n",
    "engine = get_vllm_engine(\n",
    "    INTERPRETER_MODEL,\n",
    "    gpu_memory_utilization=0.85,  # Adjust based on your GPU memory\n",
    "    tensor_parallel_size=1,        # Use multiple GPUs if available\n",
    ")\n",
    "\n",
    "print(\"vLLM engine loaded successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interpret All SAE Neurons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hypothesaes.quickstart import interpret_sae\n",
    "import pandas as pd\n",
    "\n",
    "# Configuration for interpretation\n",
    "N_NEURONS_TO_INTERPRET = sae.m_total_neurons  # Interpret all neurons\n",
    "N_EXAMPLES_FOR_INTERPRETATION = 20\n",
    "MAX_WORDS_PER_EXAMPLE = 256\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"Interpreting SAE Neurons with Local Model\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Total neurons to interpret: {N_NEURONS_TO_INTERPRET}\")\n",
    "print(f\"Examples per neuron: {N_EXAMPLES_FOR_INTERPRETATION}\")\n",
    "print(f\"Interpreter model: {INTERPRETER_MODEL}\")\n",
    "print(\"Note: HypotheSAEs will automatically use vLLM for local models\")\n",
    "\n",
    "# Interpret all neurons using local Gemma 3 12B model\n",
    "# HypotheSAEs automatically detects local models and uses vLLM\n",
    "interpretations_df = interpret_sae(\n",
    "    texts=flattened_texts,\n",
    "    embeddings=flattened_activations,\n",
    "    sae=sae,\n",
    "    n_top_neurons=N_NEURONS_TO_INTERPRET,\n",
    "    interpreter_model=INTERPRETER_MODEL,  # Local model will be used via vLLM\n",
    "    n_examples_for_interpretation=N_EXAMPLES_FOR_INTERPRETATION,\n",
    "    max_words_per_example=MAX_WORDS_PER_EXAMPLE,\n",
    "    interpret_temperature=0.7,\n",
    "    max_interpretation_tokens=50,\n",
    "    n_candidates=1,\n",
    "    print_examples_n=3,\n",
    "    print_examples_max_chars=1024,\n",
    "    task_specific_instructions=\"These are activations from a model processing cognitive action descriptions. Focus on identifying specific cognitive patterns, reasoning types, or mental processes.\",\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Interpretation Complete!\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Interpreted {len(interpretations_df)} neurons\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Interpretations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save interpretations to CSV\n",
    "OUTPUT_PATH = \"sae_neuron_interpretations.csv\"\n",
    "\n",
    "interpretations_df.to_csv(OUTPUT_PATH, index=False)\n",
    "print(f\"Interpretations saved to {OUTPUT_PATH}\")\n",
    "\n",
    "# Display summary\n",
    "print(\"\\nSample interpretations:\")\n",
    "print(interpretations_df.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Upload Interpretations to HuggingFace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import HfApi\n",
    "\n",
    "# Upload interpretations CSV to the SAE repo\n",
    "api = HfApi()\n",
    "\n",
    "print(f\"Uploading interpretations to {SAE_REPO}...\")\n",
    "api.upload_file(\n",
    "    path_or_fileobj=OUTPUT_PATH,\n",
    "    path_in_repo=\"neuron_interpretations.csv\",\n",
    "    repo_id=SAE_REPO,\n",
    "    repo_type=\"model\",\n",
    ")\n",
    "\n",
    "print(f\"✅ Interpretations uploaded to https://huggingface.co/{SAE_REPO}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
