{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test SAE on Custom Strings\n",
    "This notebook allows you to test the trained SAE on custom text inputs to see which neurons activate most strongly and what they represent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup: Load Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, \"HypotheSAEs\")\n",
    "\n",
    "from hypothesaes.sae import load_model\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from huggingface_hub import snapshot_download\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import hf_hub_download\n",
    "\n",
    "# Configuration\n",
    "SAE_REPO = \"Koalacrown/llama3.1-8b-it-cognitive-actions-sae-l11\"\n",
    "LOCAL_SAE_DIR = \"sae_checkpoint\"\n",
    "\n",
    "# Download SAE from HuggingFace\n",
    "print(f\"Downloading SAE from {SAE_REPO}...\")\n",
    "snapshot_download(\n",
    "    repo_id=SAE_REPO,\n",
    "    local_dir=LOCAL_SAE_DIR,\n",
    "    repo_type=\"model\"\n",
    ")\n",
    "\n",
    "# Load SAE\n",
    "sae_files = [f for f in os.listdir(LOCAL_SAE_DIR) if f.startswith('SAE_') and f.endswith('.pt')]\n",
    "sae_path = os.path.join(LOCAL_SAE_DIR, sae_files[0])\n",
    "print(f\"Loading SAE from {sae_path}...\")\n",
    "sae = load_model(sae_path)\n",
    "print(f\"SAE loaded: M={sae.m_total_neurons}, K={sae.k_active_neurons}\")\n",
    "\n",
    "# Download interpretations from HuggingFace\n",
    "print(f\"\\nDownloading interpretations from {SAE_REPO}...\")\n",
    "interpretations_path = hf_hub_download(\n",
    "    repo_id=SAE_REPO,\n",
    "    filename=\"neuron_interpretations.csv\",\n",
    "    repo_type=\"model\"\n",
    ")\n",
    "\n",
    "# Load interpretations\n",
    "print(f\"Loading interpretations from {interpretations_path}...\")\n",
    "interpretations_df = pd.read_csv(interpretations_path)\n",
    "print(f\"Loaded {len(interpretations_df)} neuron interpretations\")\n",
    "print(\"\\nSample interpretations:\")\n",
    "print(interpretations_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "SAE_REPO = \"Koalacrown/llama3.1-8b-it-cognitive-actions-sae-l11\"\n",
    "LOCAL_SAE_DIR = \"sae_checkpoint\"\n",
    "INTERPRETATIONS_PATH = \"sae_neuron_interpretations.csv\"\n",
    "\n",
    "# Download SAE if needed\n",
    "if not os.path.exists(LOCAL_SAE_DIR):\n",
    "    print(f\"Downloading SAE from {SAE_REPO}...\")\n",
    "    snapshot_download(\n",
    "        repo_id=SAE_REPO,\n",
    "        local_dir=LOCAL_SAE_DIR,\n",
    "        repo_type=\"model\"\n",
    "    )\n",
    "\n",
    "# Load SAE\n",
    "sae_files = [f for f in os.listdir(LOCAL_SAE_DIR) if f.startswith('SAE_') and f.endswith('.pt')]\n",
    "sae_path = os.path.join(LOCAL_SAE_DIR, sae_files[0])\n",
    "print(f\"Loading SAE from {sae_path}...\")\n",
    "sae = load_model(sae_path)\n",
    "print(f\"SAE loaded: M={sae.m_total_neurons}, K={sae.k_active_neurons}\")\n",
    "\n",
    "# Load interpretations\n",
    "print(f\"\\nLoading interpretations from {INTERPRETATIONS_PATH}...\")\n",
    "interpretations_df = pd.read_csv(INTERPRETATIONS_PATH)\n",
    "print(f\"Loaded {len(interpretations_df)} neuron interpretations\")\n",
    "print(\"\\nSample interpretations:\")\n",
    "print(interpretations_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Base Model (Llama 3.1 8B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = \"meta-llama/Llama-3.1-8B-Instruct\"\n",
    "LAYER_IDX = 11  # Layer from which SAE was trained\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "print(f\"Loading base model: {MODEL_NAME}\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, trust_remote_code=True)\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True\n",
    ")\n",
    "model.eval()\n",
    "print(f\"Model loaded on {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_activations(text, model, tokenizer, layer_idx=11, pool_method=\"mean\"):\n",
    "    \"\"\"\n",
    "    Extract activations from the base model for a given text.\n",
    "    \n",
    "    Args:\n",
    "        text: Input string\n",
    "        model: Base language model\n",
    "        tokenizer: Tokenizer\n",
    "        layer_idx: Layer to extract from\n",
    "        pool_method: How to pool across tokens (\"mean\", \"max\", or \"first\")\n",
    "    \n",
    "    Returns:\n",
    "        Pooled activation vector (numpy array)\n",
    "    \"\"\"\n",
    "    inputs = tokenizer(\n",
    "        text,\n",
    "        return_tensors=\"pt\",\n",
    "        truncation=True,\n",
    "        max_length=512,\n",
    "        padding=False,\n",
    "    ).to(model.device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs, output_hidden_states=True, return_dict=True)\n",
    "        layer_acts = outputs.hidden_states[layer_idx].squeeze(0).cpu().float().numpy()\n",
    "    \n",
    "    # Pool across tokens\n",
    "    if pool_method == \"mean\":\n",
    "        pooled = layer_acts.mean(axis=0)\n",
    "    elif pool_method == \"max\":\n",
    "        pooled = layer_acts.max(axis=0)\n",
    "    elif pool_method == \"first\":\n",
    "        pooled = layer_acts[0]\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown pool_method: {pool_method}\")\n",
    "    \n",
    "    return pooled\n",
    "\n",
    "\n",
    "def analyze_text(text, sae, model, tokenizer, interpretations_df, \n",
    "                 top_k=10, layer_idx=11, pool_method=\"mean\"):\n",
    "    \"\"\"\n",
    "    Analyze a text and show which SAE neurons activate most strongly.\n",
    "    \n",
    "    Args:\n",
    "        text: Input string to analyze\n",
    "        sae: Trained SAE model\n",
    "        model: Base language model\n",
    "        tokenizer: Tokenizer\n",
    "        interpretations_df: DataFrame with neuron interpretations\n",
    "        top_k: Number of top neurons to display\n",
    "        layer_idx: Layer to extract from\n",
    "        pool_method: How to pool activations\n",
    "    \n",
    "    Returns:\n",
    "        DataFrame with top activated neurons and their interpretations\n",
    "    \"\"\"\n",
    "    print(f\"\\nAnalyzing: \\\"{text}\\\"\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    # Step 1: Get base model activations\n",
    "    activations = get_activations(text, model, tokenizer, layer_idx, pool_method)\n",
    "    print(f\"Extracted activations: shape={activations.shape}\")\n",
    "    \n",
    "    # Step 2: Pass through SAE\n",
    "    sae_activations = sae.get_activations(\n",
    "        activations.reshape(1, -1),  # Add batch dimension\n",
    "        show_progress=False\n",
    "    ).squeeze()  # Remove batch dimension\n",
    "    print(f\"SAE activations: shape={sae_activations.shape}\")\n",
    "    \n",
    "    # Step 3: Find top-k activated neurons\n",
    "    top_indices = np.argsort(sae_activations)[::-1][:top_k]\n",
    "    top_activations = sae_activations[top_indices]\n",
    "    \n",
    "    # Step 4: Get interpretations for top neurons\n",
    "    results = []\n",
    "    for idx, activation in zip(top_indices, top_activations):\n",
    "        # Find interpretation for this neuron\n",
    "        # Check different possible column names for neuron ID\n",
    "        if 'neuron_id' in interpretations_df.columns:\n",
    "            neuron_row = interpretations_df[interpretations_df['neuron_id'] == idx]\n",
    "        elif 'neuron_index' in interpretations_df.columns:\n",
    "            neuron_row = interpretations_df[interpretations_df['neuron_index'] == idx]\n",
    "        elif 'neuron' in interpretations_df.columns:\n",
    "            neuron_row = interpretations_df[interpretations_df['neuron'] == idx]\n",
    "        else:\n",
    "            # Assume index corresponds to neuron ID\n",
    "            neuron_row = interpretations_df.iloc[[idx]] if idx < len(interpretations_df) else None\n",
    "        \n",
    "        if neuron_row is not None and len(neuron_row) > 0:\n",
    "            # Find interpretation column\n",
    "            interp_col = None\n",
    "            for col in ['interpretation', 'description', 'label']:\n",
    "                if col in neuron_row.columns:\n",
    "                    interp_col = col\n",
    "                    break\n",
    "            \n",
    "            interpretation = neuron_row[interp_col].values[0] if interp_col else \"No interpretation available\"\n",
    "        else:\n",
    "            interpretation = \"No interpretation available\"\n",
    "        \n",
    "        results.append({\n",
    "            'neuron_id': int(idx),\n",
    "            'activation': float(activation),\n",
    "            'interpretation': interpretation\n",
    "        })\n",
    "    \n",
    "    results_df = pd.DataFrame(results)\n",
    "    return results_df\n",
    "\n",
    "\n",
    "def batch_analyze(texts, sae, model, tokenizer, interpretations_df,\n",
    "                  top_k=5, layer_idx=11, pool_method=\"mean\"):\n",
    "    \"\"\"\n",
    "    Analyze multiple texts and compare their neuron activations.\n",
    "    \"\"\"\n",
    "    all_results = {}\n",
    "    \n",
    "    for i, text in enumerate(texts):\n",
    "        results_df = analyze_text(\n",
    "            text, sae, model, tokenizer, interpretations_df,\n",
    "            top_k=top_k, layer_idx=layer_idx, pool_method=pool_method\n",
    "        )\n",
    "        all_results[f\"Text {i+1}\"] = results_df\n",
    "        print(f\"\\nTop {top_k} activated neurons:\")\n",
    "        print(results_df.to_string(index=False))\n",
    "        print(\"\\n\" + \"=\" * 80)\n",
    "    \n",
    "    return all_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test on Single Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Test a single string\n",
    "test_text = \"I need to reconsider my assumptions about this problem and think more carefully.\"\n",
    "\n",
    "results = analyze_text(\n",
    "    test_text,\n",
    "    sae=sae,\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    interpretations_df=interpretations_df,\n",
    "    top_k=10,\n",
    "    pool_method=\"mean\"  # Try \"mean\", \"max\", or \"first\"\n",
    ")\n",
    "\n",
    "print(\"\\nTop 10 activated neurons:\")\n",
    "print(results.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test on Multiple Texts (Batch Analysis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Test multiple strings\n",
    "test_texts = [\n",
    "    \"I realize I was wrong about my initial hypothesis and need to revise my thinking.\",\n",
    "    \"Let me break down this complex problem into smaller, manageable steps.\",\n",
    "    \"I'm uncertain whether this approach will work, so I should test it first.\",\n",
    "    \"I need to check my biases before making this decision.\",\n",
    "]\n",
    "\n",
    "batch_results = batch_analyze(\n",
    "    test_texts,\n",
    "    sae=sae,\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    interpretations_df=interpretations_df,\n",
    "    top_k=5\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interactive Testing\n",
    "Run this cell multiple times with different inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enter your own text here\n",
    "custom_text = input(\"Enter text to analyze: \")\n",
    "\n",
    "if custom_text.strip():\n",
    "    results = analyze_text(\n",
    "        custom_text,\n",
    "        sae=sae,\n",
    "        model=model,\n",
    "        tokenizer=tokenizer,\n",
    "        interpretations_df=interpretations_df,\n",
    "        top_k=10\n",
    "    )\n",
    "    print(\"\\nTop 10 activated neurons:\")\n",
    "    print(results.to_string(index=False))\n",
    "else:\n",
    "    print(\"No text entered.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize Activation Patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "def visualize_activations(text, sae, model, tokenizer, top_k=20):\n",
    "    \"\"\"Visualize top-k neuron activations as a bar chart.\"\"\"\n",
    "    activations = get_activations(text, model, tokenizer, layer_idx=LAYER_IDX)\n",
    "    sae_activations = sae.get_activations(\n",
    "        activations.reshape(1, -1),\n",
    "        show_progress=False\n",
    "    ).squeeze()\n",
    "    \n",
    "    top_indices = np.argsort(sae_activations)[::-1][:top_k]\n",
    "    top_activations = sae_activations[top_indices]\n",
    "    \n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.bar(range(top_k), top_activations)\n",
    "    plt.xlabel('Neuron Rank')\n",
    "    plt.ylabel('Activation Value')\n",
    "    plt.title(f'Top {top_k} SAE Neuron Activations\\n\"{text[:50]}...\"')\n",
    "    plt.xticks(range(top_k), [f'N{i}' for i in top_indices], rotation=45)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "# Example usage\n",
    "visualize_activations(\n",
    "    \"I need to question my initial assumptions and consider alternative explanations.\",\n",
    "    sae, model, tokenizer, top_k=15\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare Activation Heatmap Across Multiple Texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_activations_heatmap(texts, sae, model, tokenizer, top_k=15):\n",
    "    \"\"\"Create a heatmap comparing neuron activations across multiple texts.\"\"\"\n",
    "    activation_matrix = []\n",
    "    \n",
    "    for text in texts:\n",
    "        activations = get_activations(text, model, tokenizer, layer_idx=LAYER_IDX)\n",
    "        sae_activations = sae.get_activations(\n",
    "            activations.reshape(1, -1),\n",
    "            show_progress=False\n",
    "        ).squeeze()\n",
    "        activation_matrix.append(sae_activations)\n",
    "    \n",
    "    activation_matrix = np.array(activation_matrix)\n",
    "    \n",
    "    # Get top-k most varying neurons across all texts\n",
    "    variance = activation_matrix.var(axis=0)\n",
    "    top_varying = np.argsort(variance)[::-1][:top_k]\n",
    "    \n",
    "    # Create heatmap\n",
    "    plt.figure(figsize=(14, 8))\n",
    "    sns.heatmap(\n",
    "        activation_matrix[:, top_varying].T,\n",
    "        cmap='viridis',\n",
    "        yticklabels=[f'Neuron {i}' for i in top_varying],\n",
    "        xticklabels=[f'Text {i+1}' for i in range(len(texts))],\n",
    "        cbar_kws={'label': 'Activation'}\n",
    "    )\n",
    "    plt.xlabel('Text')\n",
    "    plt.ylabel('Neuron')\n",
    "    plt.title(f'Top {top_k} Most Varying SAE Neurons Across {len(texts)} Texts')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Example usage\n",
    "compare_texts = [\n",
    "    \"I need to reconsider my initial assumptions.\",\n",
    "    \"Let me break this problem into smaller parts.\",\n",
    "    \"I'm uncertain about the best approach here.\",\n",
    "    \"I should check for biases in my reasoning.\",\n",
    "]\n",
    "\n",
    "compare_activations_heatmap(compare_texts, sae, model, tokenizer, top_k=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleanup (Optional)\n",
    "Free up GPU memory when done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment to free GPU memory\n",
    "# del model, tokenizer, sae\n",
    "# import gc\n",
    "# gc.collect()\n",
    "# if torch.cuda.is_available():\n",
    "#     torch.cuda.empty_cache()\n",
    "# print(\"Models unloaded and GPU memory freed\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
