{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SAE Training Pipeline for Cognitive Actions\n",
    "This notebook extracts activations from LLaMA-3.1-8B and trains a Sparse Autoencoder (SAE) using the FAST methodology."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup: Clone Repository and Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clone the repository\n",
    "!git clone https://github.com/ChuloIva/SAE_train_cognitive_actions.git\n",
    "%cd SAE_train_cognitive_actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clone and install HypotheSAEs separately (it's not included in the repo)\n",
    "!git clone https://github.com/rmovva/HypotheSAEs.git\n",
    "!pip install -e HypotheSAEs/\n",
    "\n",
    "# Install other dependencies\n",
    "!pip install transformers accelerate huggingface_hub"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Authenticate with HuggingFace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Extract Activations from LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import torch\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from tqdm.auto import tqdm\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "def load_dataset(dataset_path: str):\n",
    "    \"\"\"Load JSONL dataset.\"\"\"\n",
    "    data = []\n",
    "    with open(dataset_path, 'r') as f:\n",
    "        for line in f:\n",
    "            data.append(json.loads(line))\n",
    "    return data\n",
    "\n",
    "def extract_activations_sequential(\n",
    "    texts,\n",
    "    model_name=\"meta-llama/Llama-3.1-8B-Instruct\",\n",
    "    layer_idx=12,\n",
    "    max_length=512,\n",
    "    device=\"cuda\" if torch.cuda.is_available() else \"cpu\",\n",
    "):\n",
    "    \"\"\"Extract activations from LLM sequentially (FAST approach).\"\"\"\n",
    "    print(f\"Loading model: {model_name}\")\n",
    "    print(f\"Extracting from layer {layer_idx}\")\n",
    "    \n",
    "    # Load model and tokenizer\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\n",
    "        model_name,\n",
    "        trust_remote_code=True\n",
    "    )\n",
    "    \n",
    "    if tokenizer.pad_token is None:\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "    \n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_name,\n",
    "        torch_dtype=torch.float16,\n",
    "        device_map=\"auto\",\n",
    "        trust_remote_code=True\n",
    "    )\n",
    "    model.eval()\n",
    "    \n",
    "    hidden_dim = model.config.hidden_size\n",
    "    print(f\"Hidden dimension: {hidden_dim}\")\n",
    "    \n",
    "    all_activations = []\n",
    "    seq_lengths = []\n",
    "    \n",
    "    print(f\"Processing {len(texts)} examples sequentially...\")\n",
    "    with torch.no_grad():\n",
    "        for text in tqdm(texts, desc=\"Extracting activations\"):\n",
    "            inputs = tokenizer(\n",
    "                text,\n",
    "                return_tensors=\"pt\",\n",
    "                truncation=True,\n",
    "                max_length=max_length,\n",
    "                padding=False,\n",
    "            ).to(device)\n",
    "            \n",
    "            actual_length = inputs['input_ids'].shape[1]\n",
    "            seq_lengths.append(actual_length)\n",
    "            \n",
    "            outputs = model(\n",
    "                **inputs,\n",
    "                output_hidden_states=True,\n",
    "                return_dict=True\n",
    "            )\n",
    "            \n",
    "            layer_activations = outputs.hidden_states[layer_idx]\n",
    "            layer_activations = layer_activations.squeeze(0).cpu().float().numpy()\n",
    "            \n",
    "            all_activations.append(layer_activations)\n",
    "            \n",
    "            del inputs, outputs, layer_activations\n",
    "            if torch.cuda.is_available():\n",
    "                torch.cuda.empty_cache()\n",
    "    \n",
    "    print(f\"Extracted activations from {len(all_activations)} examples\")\n",
    "    print(f\"Sequence lengths: min={min(seq_lengths)}, max={max(seq_lengths)}, mean={np.mean(seq_lengths):.1f}\")\n",
    "    \n",
    "    return all_activations, seq_lengths\n",
    "\n",
    "def pad_activations(activations_list, max_length=None, pad_value=0.0):\n",
    "    \"\"\"Pad variable-length activations to same length.\"\"\"\n",
    "    n_examples = len(activations_list)\n",
    "    hidden_dim = activations_list[0].shape[1]\n",
    "    \n",
    "    if max_length is None:\n",
    "        max_length = max(a.shape[0] for a in activations_list)\n",
    "    \n",
    "    padded = np.full((n_examples, max_length, hidden_dim), pad_value, dtype=np.float32)\n",
    "    mask = np.zeros((n_examples, max_length), dtype=bool)\n",
    "    \n",
    "    for i, acts in enumerate(activations_list):\n",
    "        seq_len = min(acts.shape[0], max_length)\n",
    "        padded[i, :seq_len, :] = acts[:seq_len, :]\n",
    "        mask[i, :seq_len] = True\n",
    "    \n",
    "    return padded, mask\n",
    "\n",
    "def flatten_activations(padded_activations, padding_mask, exclude_padding=True):\n",
    "    \"\"\"Flatten sequence dimension for SAE training.\"\"\"\n",
    "    if exclude_padding:\n",
    "        mask_expanded = padding_mask[:, :, np.newaxis]\n",
    "        flattened = padded_activations[mask_expanded.squeeze(-1)]\n",
    "    else:\n",
    "        n_examples, seq_len, hidden_dim = padded_activations.shape\n",
    "        flattened = padded_activations.reshape(n_examples * seq_len, hidden_dim)\n",
    "    \n",
    "    return flattened"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Configuration\nDATASET_PATH = \"cognitive_actions_7k_final_1759233061.jsonl\"\nMODEL_NAME = \"meta-llama/Llama-3.1-8B-Instruct\"\nLAYER_IDX = 11\nMAX_LENGTH = 512\nACTIVATIONS_MMAP_PATH = \"activations.mmap\"\n\n# Load dataset\nprint(f\"Loading dataset from {DATASET_PATH}\")\ndata = load_dataset(DATASET_PATH)\ntexts = [item['text'] for item in data]\nprint(f\"Loaded {len(texts)} examples\")\n\n# Extract activations\nactivations_list, seq_lengths = extract_activations_sequential(\n    texts=texts,\n    model_name=MODEL_NAME,\n    layer_idx=LAYER_IDX,\n    max_length=MAX_LENGTH,\n)\n\n# Pad and flatten\nprint(\"\\nPadding activations...\")\npadded_activations, padding_mask = pad_activations(activations_list)\nprint(f\"Padded shape: {padded_activations.shape}\")\n\nprint(\"\\nFlattening activations (excluding padding positions)...\")\nflattened_activations = flatten_activations(\n    padded_activations,\n    padding_mask,\n    exclude_padding=True\n)\nprint(f\"Flattened shape: {flattened_activations.shape}\")\n\n# Save to memory-mapped file to avoid RAM limits\nprint(f\"\\nSaving activations to memory-mapped file: {ACTIVATIONS_MMAP_PATH}\")\nactivations_shape = flattened_activations.shape\nactivations_mmap = np.memmap(\n    ACTIVATIONS_MMAP_PATH, \n    dtype='float32', \n    mode='w+', \n    shape=activations_shape\n)\nactivations_mmap[:] = flattened_activations[:]\nactivations_mmap.flush()\n\n# Free RAM\ndel flattened_activations, padded_activations, activations_list\nprint(\"Activations saved to disk, RAM freed\")\n\nprint(\"\\n\" + \"=\"*60)\nprint(\"SUMMARY\")\nprint(\"=\"*60)\nprint(f\"Model: {MODEL_NAME}\")\nprint(f\"Layer: {LAYER_IDX}\")\nprint(f\"Examples: {len(texts)}\")\nprint(f\"Total tokens (excl. padding): {activations_shape[0]:,}\")\nprint(f\"Hidden dimension: {activations_shape[1]}\")\nprint(f\"Avg tokens per example: {activations_shape[0] / len(texts):.1f}\")\nprint(f\"Memmap file: {ACTIVATIONS_MMAP_PATH}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Train Sparse Autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import sys\nsys.path.insert(0, \"HypotheSAEs\")\n\nfrom hypothesaes.sae import SparseAutoencoder, get_sae_checkpoint_name, load_model\nfrom torch.utils.data import Dataset, DataLoader\nimport numpy as np\nimport torch\nimport os\nimport types\n\n# Memory-mapped dataset class\nclass MemmapDataset(Dataset):\n    \"\"\"Dataset that loads activations from a memory-mapped file.\"\"\"\n    \n    def __init__(self, mmap_path: str, shape: tuple, dtype='float32'):\n        self.data = np.memmap(mmap_path, dtype=dtype, mode='r', shape=shape)\n    \n    def __len__(self):\n        return self.data.shape[0]\n    \n    def __getitem__(self, idx):\n        return torch.from_numpy(self.data[idx].copy()).float()\n\n# Custom fit method for DataLoader-based training\ndef fit_with_loader(\n    self,\n    train_loader,\n    val_loader=None,\n    save_dir=None,\n    learning_rate: float = 5e-4,\n    n_epochs: int = 100,\n    aux_coef: float = 1/32,\n    multi_coef: float = 0.0,\n    patience: int = 5,\n    show_progress: bool = True,\n    clip_grad: float = 1.0\n):\n    \"\"\"Train using DataLoader objects.\"\"\"\n    from tqdm.auto import tqdm\n    \n    # Initialize weights from first batch\n    first_batch = next(iter(train_loader))\n    self.initialize_weights_(first_batch.to(self.device))\n    \n    optimizer = torch.optim.Adam(self.parameters(), lr=learning_rate)\n    \n    best_val_loss = float('inf')\n    patience_counter = 0\n    history = {'train_loss': [], 'val_loss': [], 'dead_neuron_ratio': []}\n    \n    iterator = tqdm(range(n_epochs)) if show_progress else range(n_epochs)\n    for epoch in iterator:\n        self.train()\n        train_losses = []\n        \n        for batch_x in train_loader:\n            batch_x = batch_x.to(self.device)\n            recon, info = self(batch_x)\n            loss = self.compute_loss(batch_x, recon, info, aux_coef, multi_coef)\n            \n            optimizer.zero_grad()\n            loss.backward()\n            self.adjust_decoder_gradient_()\n            \n            if clip_grad is not None:\n                torch.nn.utils.clip_grad_norm_(self.parameters(), clip_grad)\n            \n            optimizer.step()\n            self.normalize_decoder_()\n            \n            train_losses.append(loss.item())\n        \n        avg_train_loss = np.mean(train_losses)\n        history['train_loss'].append(avg_train_loss)\n        \n        dead_ratio = (self.steps_since_activation > self.dead_neuron_threshold_steps).float().mean().item()\n        history['dead_neuron_ratio'].append(dead_ratio)\n        \n        avg_val_loss = None\n        if val_loader is not None:\n            self.eval()\n            val_losses = []\n            with torch.no_grad():\n                for batch_x in val_loader:\n                    batch_x = batch_x.to(self.device)\n                    recon, info = self(batch_x)\n                    val_loss = self.compute_loss(batch_x, recon, info, aux_coef, multi_coef)\n                    val_losses.append(val_loss.item())\n            \n            avg_val_loss = np.mean(val_losses)\n            history['val_loss'].append(avg_val_loss)\n            \n            if avg_val_loss < best_val_loss:\n                best_val_loss = avg_val_loss\n                patience_counter = 0\n            else:\n                patience_counter += 1\n                if patience_counter >= patience:\n                    if show_progress:\n                        print(f\"Early stopping triggered after {epoch+1} epochs\")\n                    break\n        \n        if show_progress:\n            postfix = {\n                'train_loss': f'{avg_train_loss:.4f}',\n                'val_loss': f'{avg_val_loss:.4f}' if val_loader else 'N/A',\n                'dead_ratio': f'{dead_ratio:.3f}'\n            }\n            if self.use_batch_topk:\n                postfix['threshold'] = f'{self.threshold.item():.2e}'\n            iterator.set_postfix(postfix)\n    \n    if save_dir is not None:\n        os.makedirs(save_dir, exist_ok=True)\n        filename = get_sae_checkpoint_name(self.m_total_neurons, self.k_active_neurons, self.prefix_lengths)\n        self.save(os.path.join(save_dir, filename))\n    \n    return history\n\ndef split_and_save_mmap(mmap_path, shape, val_ratio=0.1):\n    \"\"\"Split memory-mapped activations into train and validation files.\"\"\"\n    full_data = np.memmap(mmap_path, dtype='float32', mode='r', shape=shape)\n    \n    n_total = shape[0]\n    n_val = int(n_total * val_ratio)\n    n_train = n_total - n_val\n    \n    indices = np.random.permutation(n_total)\n    train_indices = indices[:n_train]\n    val_indices = indices[n_train:]\n    \n    train_mmap_path = mmap_path.replace('.mmap', '_train.mmap')\n    train_shape = (n_train, shape[1])\n    train_mmap = np.memmap(train_mmap_path, dtype='float32', mode='w+', shape=train_shape)\n    train_mmap[:] = full_data[train_indices]\n    train_mmap.flush()\n    del train_mmap\n    \n    val_mmap_path = mmap_path.replace('.mmap', '_val.mmap')\n    val_shape = (n_val, shape[1])\n    val_mmap = np.memmap(val_mmap_path, dtype='float32', mode='w+', shape=val_shape)\n    val_mmap[:] = full_data[val_indices]\n    val_mmap.flush()\n    del val_mmap\n    \n    print(f\"Train set: {n_train:,} tokens -> {train_mmap_path}\")\n    print(f\"Val set: {n_val:,} tokens -> {val_mmap_path}\")\n    \n    return train_mmap_path, train_shape, val_mmap_path, val_shape"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Configuration\nCHECKPOINT_DIR = \"checkpoints/cognitive_actions\"\n\n# SAE hyperparameters\nM = 256  # Total number of SAE features\nK = 8    # Active features per example\n\n# Optional: Use Matryoshka prefixes for multi-granularity features\nUSE_MATRYOSHKA = True\nMATRYOSHKA_PREFIXES = [64, 256] if USE_MATRYOSHKA else None\n\n# Training parameters\nN_EPOCHS = 100\nBATCH_SIZE = 512\nLEARNING_RATE = 5e-4\nPATIENCE = 5\nVAL_RATIO = 0.1\n\nprint(\"=\"*60)\nprint(\"FAST-style SAE Training (Memory-Mapped)\")\nprint(\"=\"*60)\n\n# Split activations into train/val mmap files\nprint(\"\\nSplitting data into train/val memory-mapped files...\")\ntrain_mmap_path, train_shape, val_mmap_path, val_shape = split_and_save_mmap(\n    ACTIVATIONS_MMAP_PATH, \n    activations_shape, \n    val_ratio=VAL_RATIO\n)\n\n# Display configuration\nprint(\"\\n\" + \"=\"*60)\nprint(\"SAE Configuration\")\nprint(\"=\"*60)\nprint(f\"M (total features): {M}\")\nprint(f\"K (active features): {K}\")\nprint(f\"Matryoshka: {USE_MATRYOSHKA}\")\nif USE_MATRYOSHKA:\n    print(f\"  Prefixes: {MATRYOSHKA_PREFIXES}\")\nprint(f\"Epochs: {N_EPOCHS}\")\nprint(f\"Batch size: {BATCH_SIZE}\")\nprint(f\"Learning rate: {LEARNING_RATE}\")\nprint(f\"Patience: {PATIENCE}\")\n\n# Check for existing checkpoint\ncheckpoint_path = None\nif CHECKPOINT_DIR is not None:\n    os.makedirs(CHECKPOINT_DIR, exist_ok=True)\n    checkpoint_name = get_sae_checkpoint_name(M, K, MATRYOSHKA_PREFIXES)\n    checkpoint_path = os.path.join(CHECKPOINT_DIR, checkpoint_name)\n    if os.path.exists(checkpoint_path):\n        print(f\"\\nLoading existing checkpoint: {checkpoint_path}\")\n        sae = load_model(checkpoint_path)\n    else:\n        checkpoint_path = None\n\nif checkpoint_path is None:\n    # Create datasets and loaders\n    train_dataset = MemmapDataset(train_mmap_path, train_shape)\n    val_dataset = MemmapDataset(val_mmap_path, val_shape)\n    \n    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=0)\n    val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=0)\n    \n    # Create SAE\n    input_dim = train_shape[1]\n    sae = SparseAutoencoder(\n        input_dim=input_dim,\n        m_total_neurons=M,\n        k_active_neurons=K,\n        aux_k=None,\n        multi_k=None,\n        dead_neuron_threshold_steps=256,\n        prefix_lengths=MATRYOSHKA_PREFIXES,\n        use_batch_topk=False,\n    )\n    \n    # Monkey-patch the fit_with_loader method\n    sae.fit_with_loader = types.MethodType(fit_with_loader, sae)\n    \n    # Train SAE\n    print(\"\\n\" + \"=\"*60)\n    print(\"Training SAE\")\n    print(\"=\"*60)\n    \n    np.random.seed(42)\n    torch.manual_seed(42)\n    if torch.cuda.is_available():\n        torch.cuda.manual_seed(42)\n    \n    sae.fit_with_loader(\n        train_loader=train_loader,\n        val_loader=val_loader,\n        save_dir=CHECKPOINT_DIR,\n        learning_rate=LEARNING_RATE,\n        n_epochs=N_EPOCHS,\n        aux_coef=1/32,\n        multi_coef=0.0,\n        patience=PATIENCE,\n        clip_grad=1.0,\n        show_progress=True,\n    )\n\nprint(\"\\n\" + \"=\"*60)\nprint(\"Training Complete!\")\nprint(\"=\"*60)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Evaluate SAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Evaluate on validation set\nprint(\"Evaluating on validation set...\")\n\n# Load validation activations from memory-mapped file\nval_activations = np.memmap(val_mmap_path, dtype='float32', mode='r', shape=val_shape)\n\n# Get SAE activations\nval_activations_sae = sae.get_activations(val_activations, show_progress=True)\n\n# Compute sparsity statistics\nsparsity = (val_activations_sae != 0).mean()\nactive_per_example = (val_activations_sae != 0).sum(axis=1).mean()\n\nprint(f\"\\nSparsity statistics:\")\nprint(f\"  Overall sparsity: {sparsity:.4f}\")\nprint(f\"  Active features per token: {active_per_example:.2f} (target: {K})\")\n\n# Compute reconstruction error\nprint(\"\\nComputing reconstruction error...\")\nval_tensor = torch.tensor(val_activations, dtype=torch.float).to(sae.device)\nwith torch.no_grad():\n    recon, info = sae(val_tensor)\n    mse = torch.nn.functional.mse_loss(recon, val_tensor).item()\n    baseline_mse = torch.nn.functional.mse_loss(\n        val_tensor.mean(dim=0, keepdim=True).expand_as(val_tensor),\n        val_tensor\n    ).item()\n    normalized_mse = mse / baseline_mse\n\nprint(f\"  MSE: {mse:.4f}\")\nprint(f\"  Normalized MSE: {normalized_mse:.4f}\")\n\n# Check for dead neurons\ndead_neurons = (sae.steps_since_activation > sae.dead_neuron_threshold_steps).sum().item()\ndead_ratio = dead_neurons / sae.m_total_neurons\n\nprint(f\"\\nDead neurons: {dead_neurons}/{sae.m_total_neurons} ({dead_ratio:.2%})\")\n\nprint(\"\\n\" + \"=\"*60)\nprint(\"SAE Training Summary\")\nprint(\"=\"*60)\nprint(f\"Checkpoint: {CHECKPOINT_DIR}\")\nprint(f\"Model: M={M}, K={K}\")\nprint(f\"Val MSE: {mse:.4f} (normalized: {normalized_mse:.4f})\")\nprint(f\"Active features/token: {active_per_example:.2f}\")\nprint(f\"Dead neurons: {dead_ratio:.2%}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Push SAE to HuggingFace Hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import HfApi, create_repo\n",
    "import shutil\n",
    "\n",
    "# Configuration - CHANGE THESE\n",
    "HF_USERNAME = \"Koalacrown\"  # Replace with your HuggingFace username\n",
    "REPO_NAME = \"llama3.1-8b-it-cognitive-actions-sae-l11\"  # Name for your SAE repo\n",
    "\n",
    "repo_id = f\"{HF_USERNAME}/{REPO_NAME}\"\n",
    "\n",
    "print(f\"Pushing SAE to HuggingFace Hub: {repo_id}\")\n",
    "\n",
    "# Create repository\n",
    "try:\n",
    "    create_repo(repo_id, repo_type=\"model\", exist_ok=True)\n",
    "    print(f\"Repository created/verified: {repo_id}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error creating repository: {e}\")\n",
    "\n",
    "# Upload checkpoint directory\n",
    "api = HfApi()\n",
    "api.upload_folder(\n",
    "    folder_path=CHECKPOINT_DIR,\n",
    "    repo_id=repo_id,\n",
    "    repo_type=\"model\",\n",
    ")\n",
    "\n",
    "print(f\"\\n✅ SAE successfully pushed to: https://huggingface.co/{repo_id}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (Optional) Create Model Card"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_card = f\"\"\"---\n",
    "license: mit\n",
    "tags:\n",
    "- sparse-autoencoder\n",
    "- interpretability\n",
    "- llama\n",
    "- cognitive-actions\n",
    "---\n",
    "\n",
    "# LLaMA-3.1-8B Cognitive Actions SAE\n",
    "\n",
    "This is a Sparse Autoencoder (SAE) trained on layer {LAYER_IDX} activations from LLaMA-3.1-8B-Instruct using the FAST methodology.\n",
    "\n",
    "## Model Details\n",
    "\n",
    "- **Base Model**: meta-llama/Llama-3.1-8B-Instruct\n",
    "- **Layer**: {LAYER_IDX}\n",
    "- **Dataset**: Cognitive Actions (7K examples)\n",
    "- **SAE Architecture**: M={M}, K={K}\n",
    "- **Methodology**: FAST (Finetuning-aligned Sequential Training)\n",
    "\n",
    "## Performance\n",
    "\n",
    "- **MSE**: {mse:.4f}\n",
    "- **Normalized MSE**: {normalized_mse:.4f}\n",
    "- **Active features/token**: {active_per_example:.2f}\n",
    "- **Dead neurons**: {dead_ratio:.2%}\n",
    "\n",
    "## Usage\n",
    "\n",
    "```python\n",
    "from hypothesaes.sae import load_model\n",
    "\n",
    "sae = load_model(\"{repo_id}\")\n",
    "features = sae.get_activations(activations)\n",
    "```\n",
    "\n",
    "## Training\n",
    "\n",
    "Trained using [HypotheSAEs](https://github.com/DavidUdell/HypotheSAEs) with the following configuration:\n",
    "\n",
    "- Epochs: {N_EPOCHS}\n",
    "- Batch size: {BATCH_SIZE}\n",
    "- Learning rate: {LEARNING_RATE}\n",
    "- Matryoshka prefixes: {MATRYOSHKA_PREFIXES}\n",
    "\n",
    "## Citation\n",
    "\n",
    "If you use this SAE, please cite the FAST methodology and HypotheSAEs.\n",
    "\"\"\"\n",
    "\n",
    "# Upload model card\n",
    "api.upload_file(\n",
    "    path_or_fileobj=model_card.encode(),\n",
    "    path_in_repo=\"README.md\",\n",
    "    repo_id=repo_id,\n",
    "    repo_type=\"model\",\n",
    ")\n",
    "\n",
    "print(\"Model card uploaded!\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}